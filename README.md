# Post-Decoding Randomized Detoxification in Language Models Text Generation

### Abstract

This project analyses the langage model GPT-J 6B’s generated texts toxicity probability distribution and takes advantage of it in order to generate detoxified texts, given a toxic or non-toxic prompt. This is the author’s final project of the IA024A (Natural Language Processing) class.

### Contents

*Randomized_Detoxification.ipynb* contains the entire code used in this project.

*prompts.7z* is the RealToxicityPrompts dataset.

*DetoxLM - Anderson Faller - Trabalho Final.pdf* details the full project development.
