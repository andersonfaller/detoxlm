{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andersonfaller/detoxlm/blob/main/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvBalPZpX3JW",
        "outputId": "b7565177-79f2-4095-8fee-cc08f5668222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.7/dist-packages (0.35.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TBdoXm5zcgxp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Type\n",
        "\n",
        "from googleapiclient import discovery\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzBo9lp8cAFQ",
        "outputId": "844a9850-1568-4fea-df00-09da3afff1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-24 03:23:40--  http://nc/\n",
            "Resolving nc (nc)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘nc’\n",
            "--2022-11-24 03:23:40--  https://github.com/andersonfaller/detoxlm/raw/main/prompts.7z\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/andersonfaller/detoxlm/main/prompts.7z [following]\n",
            "--2022-11-24 03:23:40--  https://raw.githubusercontent.com/andersonfaller/detoxlm/main/prompts.7z\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15732789 (15M) [application/octet-stream]\n",
            "Saving to: ‘prompts.7z.1’\n",
            "\n",
            "prompts.7z.1        100%[===================>]  15.00M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-11-24 03:23:40 (451 MB/s) - ‘prompts.7z.1’ saved [15732789/15732789]\n",
            "\n",
            "FINISHED --2022-11-24 03:23:40--\n",
            "Total wall clock time: 0.7s\n",
            "Downloaded: 1 files, 15M in 0.03s (451 MB/s)\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 15732789 bytes (16 MiB)\n",
            "\n",
            "Extracting archive: prompts.7z\n",
            "--\n",
            "Path = prompts.7z\n",
            "Type = 7z\n",
            "Physical Size = 15732789\n",
            "Headers Size = 130\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b\n",
            "Would you like to replace the existing file:\n",
            "  Path:     ./prompts.jsonl\n",
            "  Size:     67690996 bytes (65 MiB)\n",
            "  Modified: 2020-10-04 01:04:48\n",
            "with the file from archive:\n",
            "  Path:     prompts.jsonl\n",
            "  Size:     67690996 bytes (65 MiB)\n",
            "  Modified: 2020-10-04 01:04:48\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? N\n",
            "\n",
            "  0% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% . prompts.jsonl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       67690996\n",
            "Compressed: 15732789\n"
          ]
        }
      ],
      "source": [
        "!wget nc https://github.com/andersonfaller/detoxlm/raw/main/prompts.7z\n",
        "!7z e prompts.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W0y7aruRjAI",
        "outputId": "aa904962-af9d-4e3c-94b9-e2e85b3322bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6vnFy_BbnhI"
      },
      "outputs": [],
      "source": [
        "API_KEY = input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQQ1aEv4ntSq",
        "outputId": "49a36781-a099-421a-a0c0-afe7d7017c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ut1IXyhjclLG"
      },
      "outputs": [],
      "source": [
        "# Para desenvolvimento, sugerimos usar o GPT menor, com 1.3B parametros.\n",
        "#model = transformers.GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", device_map=\"auto\", load_in_8bit=True)\n",
        "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Se estiver usando o colab pro, conseguimos rodar um GPT-J de 6B de parametros.\n",
        "model = transformers.GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVp-y0xKsp4I",
        "outputId": "085c1f86-db54-41de-fd02-6b3db1d9d929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert_model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\").to(device)\n",
        "bert_model = bert_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0fTiDdds90R",
        "outputId": "1b3644ec-3f42-4fa5-81f9-bebcb6a31b33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "filename = '/content/drive/MyDrive/' + 'bert_perspective.pt'\n",
        "bert_model.load_state_dict(pickle.load(open(filename, 'rb')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6xIkVH5ZN_3y"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"This bastard came to me asking for something I couldn't\"\n",
        "    )\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    inputs = tokens.input_ids.to(device),\n",
        "    attention_mask = tokens.attention_mask.to(device),\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "    do_sample = True,\n",
        "    temperature = 0.9,\n",
        "    max_length = len(tokens.input_ids[0]) + 20,\n",
        "    num_return_sequences = 10\n",
        ")\n",
        "\n",
        "gen_texts = tokenizer.batch_decode(gen_tokens)\n",
        "continuations = [text[len(prompt):].strip() for text in gen_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sp2B-zVSto1H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P7JZFoHSTZ5w"
      },
      "outputs": [],
      "source": [
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  #static_discovery=False,\n",
        ")\n",
        "\n",
        "def analyse_toxicity_bert(texts):\n",
        "  inputs = bert_tokenizer.batch_encode_plus(texts, max_length = 50, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    logits = bert_model(**inputs)\n",
        "  probs = torch.softmax(logits.logits, dim = -1)\n",
        "  return probs[:,0].cpu().numpy()\n",
        "\n",
        "\n",
        "def analyse_toxicity(text):\n",
        "  analyze_request = {\n",
        "    'comment': { 'text': text },\n",
        "    'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "  try:\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "    toxicity = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "  except:\n",
        "    print('ERRO')\n",
        "    return np.nan\n",
        "  return toxicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwsGl_DvuMcu",
        "outputId": "a7e524d7-aa78-40d1-c00a-44862e1a81e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9600472 , 0.07353213], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "analyse_toxicity_bert(['Fuck you', 'Hey, handsome'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Prezc34d9r6t"
      },
      "outputs": [],
      "source": [
        "def my_multinomial(weights):\n",
        "   total = sum(weights)\n",
        "   r = random.uniform(0, total)\n",
        "   posicao = 0\n",
        "   for i, w in enumerate(weights):\n",
        "      if posicao + w >= r:\n",
        "         return i\n",
        "      posicao += w\n",
        "\n",
        "def decode_sampling(logits, temperature = 1):\n",
        "  probs = torch.softmax(logits/temperature, dim = -1)\n",
        "  return my_multinomial(probs)\n",
        "\n",
        "def decode_top_k(logits, temperature = 1, k = 1):\n",
        "  probs = torch.softmax(logits/temperature, dim = -1)\n",
        "  top_k = probs.topk(k)\n",
        "  return top_k.indices[my_multinomial(top_k.values)].item()\n",
        "\n",
        "def decode_top_p(logits, temperature = 1, p = 1):\n",
        "  probs = torch.softmax(logits/temperature, dim = -1)\n",
        "  top_p_idx = []\n",
        "  top_p_values = []\n",
        "  idx = torch.argsort(logits, descending = True)\n",
        "  for i in idx:\n",
        "    top_p_idx.append(i)\n",
        "    top_p_values.append(probs[i])\n",
        "    if sum(top_p_values) >= p:\n",
        "      break\n",
        "  return top_p_idx[my_multinomial(top_p_values)].item()\n",
        "\n",
        "def generate(model, prompt, max_length, temperature = 1, top_k = -1, top_p = -1):\n",
        "  if top_k > 0:\n",
        "    decoder = lambda x: decode_top_k(x, temperature, top_k)\n",
        "  elif top_p > 0:\n",
        "    decoder = lambda x: decode_top_p(x, temperature, top_p)\n",
        "  else:\n",
        "    decoder = lambda x: decode_sampling(x, temperature)\n",
        "  \n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
        "\n",
        "  for i in range(max_length):\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids.cuda()).logits[-1] # último token\n",
        "    next_token = decoder(logits)\n",
        "    input_ids = torch.cat([input_ids, torch.LongTensor([next_token])])\n",
        "  \n",
        "  return tokenizer.decode(input_ids)\n",
        "\n",
        "def generate_modified(model, prompt, max_length, temperature = 1, top_k = -1, top_p = -1):\n",
        "  if top_k > 0:\n",
        "    decoder = lambda x: decode_top_k(x, temperature, top_k)\n",
        "  elif top_p > 0:\n",
        "    decoder = lambda x: decode_top_p(x, temperature, top_p)\n",
        "  else:\n",
        "    decoder = lambda x: decode_sampling(x, temperature)\n",
        "  \n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
        "\n",
        "  for i in range(max_length):\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids.cuda()).logits[-1] # último token\n",
        "    next_token = decoder(logits)\n",
        "    input_ids = torch.cat([input_ids, torch.LongTensor([next_token])])\n",
        "  \n",
        "  return tokenizer.decode(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "37JVW56mbtLT"
      },
      "outputs": [],
      "source": [
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  #static_discovery=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Eu_SAqb0b116"
      },
      "outputs": [],
      "source": [
        "with open('prompts.jsonl') as file:\n",
        "  prompts = [json.loads(line) for line in file.readlines()]\n",
        "\n",
        "df_prompts = pd.DataFrame.from_records([p['prompt'] for p in prompts])\n",
        "df_continuations = pd.DataFrame.from_records([p['continuation'] for p in prompts])\n",
        "series_challenging = pd.DataFrame.from_records(prompts)['challenging']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v4ZuSRIy1t8"
      },
      "outputs": [],
      "source": [
        "#df_my_continuations = pd.DataFrame()\n",
        "df_my_continuations = pickle.load(open('/content/drive/MyDrive/df_my_continuations_k1.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMF4Y8YxcSDD",
        "outputId": "36de0bba-7ff3-4d23-8c63-a1d0a358a8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "contador = 20; i = 85590\n",
            "contador = 40; i = 47416\n",
            "contador = 60; i = 44937\n",
            "contador = 80; i = 86491\n",
            "contador = 100; i = 8432\n",
            "contador = 120; i = 76545\n",
            "contador = 140; i = 66541\n",
            "contador = 160; i = 99143\n",
            "contador = 180; i = 34207\n",
            "contador = 200; i = 34027\n",
            "contador = 220; i = 59154\n",
            "contador = 240; i = 94185\n",
            "contador = 260; i = 54010\n",
            "contador = 280; i = 66341\n",
            "contador = 300; i = 17284\n",
            "contador = 320; i = 61008\n",
            "contador = 340; i = 98151\n",
            "contador = 360; i = 86262\n",
            "contador = 380; i = 63446\n",
            "contador = 400; i = 22752\n",
            "contador = 420; i = 47888\n",
            "contador = 440; i = 77043\n",
            "contador = 460; i = 95496\n",
            "contador = 480; i = 17116\n",
            "contador = 500; i = 59071\n",
            "contador = 520; i = 28721\n",
            "contador = 540; i = 30018\n",
            "contador = 560; i = 66173\n",
            "contador = 580; i = 28393\n",
            "contador = 600; i = 62169\n",
            "contador = 620; i = 36917\n",
            "contador = 640; i = 54841\n",
            "contador = 660; i = 63815\n",
            "contador = 680; i = 16505\n",
            "contador = 700; i = 71284\n",
            "contador = 720; i = 87754\n",
            "contador = 740; i = 30966\n",
            "contador = 760; i = 43773\n",
            "contador = 780; i = 47996\n",
            "contador = 800; i = 33201\n",
            "contador = 820; i = 36337\n",
            "contador = 840; i = 38007\n",
            "contador = 860; i = 44944\n",
            "contador = 880; i = 28015\n",
            "contador = 900; i = 60949\n",
            "contador = 920; i = 19017\n",
            "contador = 940; i = 54489\n",
            "contador = 960; i = 48934\n",
            "contador = 980; i = 1845\n",
            "contador = 1000; i = 54406\n"
          ]
        }
      ],
      "source": [
        "max_length = 20\n",
        "num_sequences = 1\n",
        "\n",
        "#for i, row in df_prompts.iterrows():\n",
        "contador = 1\n",
        "while True: \n",
        "  i = random.choice(df_prompts.index)\n",
        "  if i in df_my_continuations.index:\n",
        "    continue\n",
        "  \n",
        "  row = df_prompts.loc[i]\n",
        "\n",
        "  prompt = row['text']\n",
        "  tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "  gen_tokens = model.generate(\n",
        "      inputs = tokens.input_ids.to(device),\n",
        "      attention_mask = tokens.attention_mask.to(device),\n",
        "      top_k = 1, # remember to change this\n",
        "      pad_token_id = tokenizer.eos_token_id,\n",
        "      do_sample = True,\n",
        "      temperature = 0.9,\n",
        "      max_length = len(tokens.input_ids[0]) + max_length,\n",
        "      num_return_sequences = num_sequences\n",
        "  )\n",
        "\n",
        "  gen_texts = tokenizer.batch_decode(gen_tokens)\n",
        "  continuations = [text[len(prompt):].strip() for text in gen_texts]\n",
        "\n",
        "  prompt_toxicity = analyse_toxicity_bert([prompt])[0]\n",
        "  #continuations_toxicity = [analyse_toxicity(c) for c in continuations]\n",
        "  continuations_toxicity = analyse_toxicity_bert(continuations)\n",
        "\n",
        "  df_my_continuations.loc[i, 'prompt'] = row['text']\n",
        "  df_my_continuations.loc[i, 'prompt_toxicity'] = prompt_toxicity\n",
        "\n",
        "  df_my_continuations.loc[i, 'min_continuation_toxicity'] = min(continuations_toxicity)\n",
        "  df_my_continuations.loc[i, 'argmin_continuation_toxicity'] = np.argmin(continuations_toxicity)\n",
        "\n",
        "  df_my_continuations.loc[i, 'max_continuation_toxicity'] = max(continuations_toxicity)\n",
        "  df_my_continuations.loc[i, 'argmax_continuation_toxicity'] = np.argmax(continuations_toxicity)\n",
        "\n",
        "  for j, continuation, continuation_toxicity in zip(range(num_sequences), continuations, continuations_toxicity):\n",
        "    df_my_continuations.loc[i, f'continuation_{j}'] = continuation\n",
        "    df_my_continuations.loc[i, f'continuation_toxicity_{j}'] = continuation_toxicity\n",
        "\n",
        "  if contador%20 == 0:\n",
        "    print(f'contador = {contador}; i = {i}')\n",
        "    pickle.dump(df_my_continuations, open('/content/drive/MyDrive/df_my_continuations_k1.pkl', 'wb'))\n",
        "\n",
        "  #print(i, prompt_toxicity, continuation_toxicity)\n",
        "  contador += 1\n",
        "  if contador > 1000:\n",
        "    break\n",
        "\n",
        "pickle.dump(df_my_continuations, open('/content/drive/MyDrive/df_my_continuations_k1.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3lwWVUAzEMu",
        "outputId": "607f850d-6b2a-4c39-ab2a-a79dcbafac12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The satanic book is on the table. He wants to know why this book is so important? I reply that it's the only book in the library that tells him the truth.\n",
            "\n",
            "\"It's forbidden, why is it?\" He points at the book.\n",
            "\n",
            "\"It's the only one I have, it's the\n",
            "The satanic book is on the table. There are two other men in the room. One is a young one and the other is an old one. But they are both fat. And it feels like the room is a lot smaller. The young man says to the boys, ‘Hey you fat bastards, get us some water\n"
          ]
        }
      ],
      "source": [
        "def generate_controlled_toxicity(prompt, num_tokens, num_sequences = 10, max_step_len = 20, min_toxicity = True):\n",
        "\n",
        "  while num_tokens > 0:\n",
        "    max_length = min(num_tokens, max_step_len)\n",
        "    num_tokens = max(0, num_tokens - max_length)\n",
        "\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    gen_tokens = model.generate(\n",
        "        inputs = tokens.input_ids.to(device),\n",
        "        attention_mask = tokens.attention_mask.to(device),\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "        do_sample = True,\n",
        "        temperature = 0.9,\n",
        "        max_length = len(tokens.input_ids[0]) + max_length,\n",
        "        num_return_sequences = num_sequences\n",
        "    )\n",
        "\n",
        "    gen_texts = tokenizer.batch_decode(gen_tokens)\n",
        "    continuations = [text[len(prompt):].strip() for text in gen_texts]\n",
        "    continuations_toxicity = analyse_toxicity_bert(continuations)\n",
        "\n",
        "    if min_toxicity:\n",
        "      idx = np.argmin(continuations_toxicity)\n",
        "    else:\n",
        "      idx = np.argmax(continuations_toxicity)\n",
        "    \n",
        "    prompt = gen_texts[idx]\n",
        "  \n",
        "  return prompt\n",
        "\n",
        "prompt = \"The satanic book is on the table\"\n",
        "\n",
        "print(generate_controlled_toxicity(prompt, 60, num_sequences = 15, max_step_len = 30, min_toxicity = True))\n",
        "print(generate_controlled_toxicity(prompt, 60, num_sequences = 15, max_step_len = 30, min_toxicity = False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOc4HjXfY98n4Ua3j0veb1l",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}