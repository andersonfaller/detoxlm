{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWdcgJ7KZ0KpC+qIVy+9G4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andersonfaller/detoxlm/blob/main/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvBalPZpX3JW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Type"
      ],
      "metadata": {
        "id": "TBdoXm5zcgxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para desenvolvimento, sugerimos usar o GPT menor, com 1.3B parametros.\n",
        "#model = transformers.GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", device_map=\"auto\", load_in_8bit=True)\n",
        "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Se estiver usando o colab pro, conseguimos rodar um GPT-J de 6B de parametros.\n",
        "model = transformers.GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ],
      "metadata": {
        "id": "ut1IXyhjclLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}